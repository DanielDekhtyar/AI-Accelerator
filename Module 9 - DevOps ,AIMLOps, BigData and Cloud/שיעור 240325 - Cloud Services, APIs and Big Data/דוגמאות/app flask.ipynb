{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee0b3e6-d026-4369-803f-df234d3420d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923273aa-b455-484e-8f9f-32a749550420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ×©×œ×‘ 1 â€“ ×™×¦×™×¨×ª ×”××•×“×œ (×ª×”×œ×™×š ×”××™××•×Ÿ)\n",
    "X = np.array([50, 75, 100, 120, 150]).reshape(-1, 1)\n",
    "y = np.array([500000, 750000, 1000000, 1200000, 1500000])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# ×©×œ×‘ 2 â€“ ×”×’×“×¨×ª ××¤×œ×™×§×¦×™×™×ª Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict_price')\n",
    "def predict_price():\n",
    "    # ×§×¨×™××ª ×”×¢×¨×š ××ª×•×š ×”-URL: ×œ××©×œ /predict_price?size=85\n",
    "    size = request.args.get('size', default=None, type=float)\n",
    "\n",
    "    if size is None:\n",
    "        return jsonify({'error': 'Missing size parameter (e.g. /predict_price?size=85)'}), 400\n",
    "\n",
    "    prediction = model.predict([[size]])[0]\n",
    "    return jsonify({'size': size, 'estimated_price': round(prediction, 2)})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d04fc0-b288-4ca8-9b9b-0cfb812fbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://127.0.0.1:5000/predict_price?size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0929c0-b17a-4f38-9030-d91eea995b6f",
   "metadata": {},
   "source": [
    "# spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8b26e-8140-41d9-aa9d-05c96190c2d6",
   "metadata": {},
   "source": [
    " ×–×” ×œ× ×¢×•×‘×“ ×›×™ ××™×Ÿ ×œ×™ java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a734a06-acf6-4917-abae-0ebd4836a90b",
   "metadata": {},
   "source": [
    "# ğŸ§  ×¡×™×›×•× ××¡×•×“×¨: Spark ×•-PySpark\n",
    "\n",
    "## ğŸ”¥ ××” ×–×” Apache Spark?\n",
    "- ×× ×•×¢ ×¢×™×‘×•×“ × ×ª×•× ×™× **××‘×•×–×¨** (Distributed Processing)\n",
    "- × ×•×¢×“ ×œ×¢×‘×•×“×” ×¢× **Big Data** â€“ × ×ª×•× ×™× ×©×œ× × ×›× ×¡×™× ×œ×–×™×›×¨×•×Ÿ\n",
    "- ××¡×•×’×œ ×œ×”×¨×™×¥ ×—×™×©×•×‘×™× ×‘××§×‘×™×œ ×¢×œ ××©×›×•×œ ××—×©×‘×™× (××• ×›××” ×œ×™×‘×•×ª)\n",
    "- ×§×•×“ ×¤×ª×•×— â€“ ×¤×¨×•×™×§×˜ ×©×œ Apache Foundation\n",
    "\n",
    "## ğŸ ××” ×–×” PySpark?\n",
    "- ×”×××©×§ ×©×œ Spark ×‘×©×¤×ª **Python**\n",
    "- ×××¤×©×¨ ×œ×›×ª×•×‘ ×§×•×“ Python ×¨×’×™×œ ×©×× ×¦×œ ××ª ×× ×•×¢ ×”×¢×™×‘×•×“ ×©×œ Spark\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ ×¢×§×¨×•× ×•×ª ×¢×‘×•×“×” ×—×©×•×‘×™× ×‘-Spark:\n",
    "\n",
    "### 1. SparkSession\n",
    "- \"××¨×›×– ×”×¤×™×§×•×“\" ×©×œ ××¤×œ×™×§×¦×™×™×ª Spark\n",
    "- ×“×¨×›×• ×™×•×¦×¨×™× DataFrame, ×˜×•×¢× ×™× ×§×‘×¦×™× ×•××¤×¢×™×œ×™× ××ª ×”×× ×•×¢\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "```\n",
    "\n",
    "### 2. Transformations (×˜×¨× ×¡×¤×•×¨××¦×™×•×ª)\n",
    "- ×¤×¢×•×œ×•×ª ×©××ª××¨×•×ª ××” ×œ×¢×©×•×ª ×¢×œ ×”× ×ª×•× ×™× (×›××• filter, groupBy)\n",
    "- **×œ× ××ª×‘×¦×¢×•×ª ×‘×¤×•×¢×œ** â€“ ×¨×§ × ×‘× ×” ×’×¨×£ ×©×œ ×¤×¢×•×œ×•×ª\n",
    "```python\n",
    "df_filtered = df.filter(df[\"age\"] > 30)  # ×¢×“×™×™×Ÿ ×œ× ×‘×•×¦×¢\n",
    "```\n",
    "\n",
    "### 3. Actions (×¤×¢×•×œ×•×ª ×¡×•×¤×™×•×ª)\n",
    "- ×’×•×¨××•×ª ×œ-Spark ×œ×”×¨×™×¥ ××ª ×›×œ ××” ×©×ª×™××¨× ×• ×œ×¤× ×™ ×›×Ÿ\n",
    "- ×“×•×’×××•×ª: `show()`, `count()`, `collect()`\n",
    "```python\n",
    "df_filtered.count()  # ×¢×›×©×™×• ××ª×‘×¦×¢ ×‘×¤×•×¢×œ\n",
    "```\n",
    "\n",
    "### 4. Lazy Evaluation (×”×¢×¨×›×” ×¢×¦×œ×”)\n",
    "- Spark **×œ× ××¨×™×¥ ×›×œ×•×** ×¢×“ ×©×¦×¨×™×š ×ª×•×¦××” ×¡×•×¤×™×ª (Action)\n",
    "- ×××¤×©×¨ ×œ×• ×œ×‘×¦×¢ ××•×¤×˜×™××™×–×¦×™×” ×•×œ×¢×‘×•×“ ×™×¢×™×œ ×™×•×ª×¨\n",
    "\n",
    "### 5. Partitions (××—×™×¦×•×ª)\n",
    "- Spark ××—×œ×§ ××ª ×”×“××˜×” ×œ×™×—×™×“×•×ª ×¢×‘×•×“×” (Partitions)\n",
    "- ××¨×™×¥ ×›×œ ×—×œ×§ ×‘××§×‘×™×œ (×× ×™×© ×›××” ×œ×™×‘×•×ª/××›×•× ×•×ª)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Spark ×œ×¢×•××ª Pandas:\n",
    "\n",
    "| × ×•×©×              | Pandas                           | Spark                            |\n",
    "|-------------------|----------------------------------|----------------------------------|\n",
    "| ×˜×¢×™× ×” ×œ×–×™×›×¨×•×Ÿ     | ×›×œ ×”×“××˜×” × ×˜×¢×Ÿ ×‘×‘×ª ××—×ª          | × ×˜×¢×Ÿ ×‘××—×™×¦×•×ª (Partitions)       |\n",
    "| ×’×•×“×œ ×“××˜×” ××ª××™×   | ×¢×“ ~1GB (×ª×œ×•×™ ×‘×–×™×›×¨×•×Ÿ)          | ××ª××™× ×’× ×œ×¢×©×¨×•×ª/×××•×ª GB         |\n",
    "| ××•×¤×Ÿ ×‘×™×¦×•×¢        | ××™×™×“×™ (Eager Execution)         | Lazy Evaluation (×“×—×•×™)           |\n",
    "| ×¨×™×¦×”              | ×œ×™×‘×” ××—×ª, ××—×©×‘ ××§×•××™            | ××§×‘×™×œ×™, ×ª×•××š ×‘-Cluster           |\n",
    "| ×‘×™×¦×•×¢×™× ×‘××—×©×‘ ××™×©×™| ××”×™×¨ ×××•×“ ×¢×œ ×§×‘×¦×™× ×§×˜× ×™×        | ××™×˜×™ ×™×—×¡×™×ª (×‘×’×œ×œ overhead)       |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª ××ª×™ ×œ×”×©×ª××© ×‘-Spark?\n",
    "- ×›×©×™×© **×“××˜×” ×¢×¦×•×** ×©×œ× × ×›× ×¡ ×œ×–×™×›×¨×•×Ÿ\n",
    "- ×›×©×¢×•×‘×“×™× ×‘×¢× ×Ÿ ××• ×‘Ö¾Cluster\n",
    "- ×›×©×¦×¨×™×š ×¢×™×‘×•×“ ××§×‘×™×œ×™, ××”×™×¨ ×•×××™×Ÿ ×‘×§× ×” ××™×“×” ×’×“×•×œ\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ ×”×¢×¨×” ×—×©×•×‘×”:\n",
    "> Spark **××™×˜×™ ×™×•×ª×¨ ××¤× ×“×¡ ×‘××—×©×‘ ××™×©×™** ×¢×œ ×§×‘×¦×™× ×§×˜× ×™×. ×”×•× ×¤×•×¨×— ×¨×§ ×›×©×¢×•×‘×“×™× ×¢× Big Data ×××™×ª×™ ×‘×¡×‘×™×‘×•×ª ××‘×•×–×¨×•×ª.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ×“×•×’××” ×§×¦×¨×”:\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 17), (\"Charlie\", 12)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# ×˜×¨× ×¡×¤×•×¨××¦×™×”\n",
    "filtered = df.filter(df[\"age\"] < 18)\n",
    "\n",
    "# ×¤×¢×•×œ×” ×¡×•×¤×™×ª\n",
    "print(filtered.count())\n",
    "```\n",
    "\n",
    "×¨×•×¦×” ×©× ×•×¡×™×£ ×’× ×“×•×’××” ×¢× groupBy ××• ×”×¦×¦×” ×œ-SparkML ×‘×”××©×š?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a89b8-2e66-47cf-8172-ff89e0dca625",
   "metadata": {},
   "source": [
    "# ğŸ§  ×¡×™×›×•× ××¡×•×“×¨: Spark ×•-PySpark\n",
    "\n",
    "## ğŸ”¥ ××” ×–×” Apache Spark?\n",
    "- ×× ×•×¢ ×¢×™×‘×•×“ × ×ª×•× ×™× **××‘×•×–×¨** (Distributed Processing)\n",
    "- × ×•×¢×“ ×œ×¢×‘×•×“×” ×¢× **Big Data** â€“ × ×ª×•× ×™× ×©×œ× × ×›× ×¡×™× ×œ×–×™×›×¨×•×Ÿ\n",
    "- ××¡×•×’×œ ×œ×”×¨×™×¥ ×—×™×©×•×‘×™× ×‘××§×‘×™×œ ×¢×œ ××©×›×•×œ ××—×©×‘×™× (××• ×›××” ×œ×™×‘×•×ª)\n",
    "- ×§×•×“ ×¤×ª×•×— â€“ ×¤×¨×•×™×§×˜ ×©×œ Apache Foundation\n",
    "\n",
    "## ğŸ ××” ×–×” PySpark?\n",
    "- ×”×××©×§ ×©×œ Spark ×‘×©×¤×ª **Python**\n",
    "- ×××¤×©×¨ ×œ×›×ª×•×‘ ×§×•×“ Python ×¨×’×™×œ ×©×× ×¦×œ ××ª ×× ×•×¢ ×”×¢×™×‘×•×“ ×©×œ Spark\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§± ××‘× ×” Spark â€“ ×©×œ×•×© ×©×›×‘×•×ª ×¢×™×§×¨×™×•×ª:\n",
    "\n",
    "### 1ï¸âƒ£ Spark Core\n",
    "- ×”×œ×‘ ×©×œ Spark: ××—×¨××™ ×¢×œ ×¢×™×‘×•×“ ××‘×•×–×¨, × ×™×”×•×œ ×–×™×›×¨×•×Ÿ ×•××©××‘×™×\n",
    "- ×›×•×œ×œ ×× ×•×¢×™× ×›××•:\n",
    "  - **Tungsten** â€“ ×× ×•×¢ ×‘×™×¦×•×¢×™× ×‘×–×™×›×¨×•×Ÿ\n",
    "  - **Catalyst** â€“ ×× ×•×¢ ××•×¤×˜×™××™×–×¦×™×” ×œ×©××™×œ×ª×•×ª\n",
    "\n",
    "### 2ï¸âƒ£ APIs â€“ ×’×™×©×•×ª ×œ×“××˜×”\n",
    "| API         | ×ª×™××•×¨                                  |\n",
    "|--------------|------------------------------------------|\n",
    "| `RDD`        | ××‘× ×” ×‘×¡×™×¡×™, ×’××™×© ××š ×¤×—×•×ª × ×•×—           |\n",
    "| `DataFrame`  | ×˜×‘×œ×” ×¢× ×¢××•×“×•×ª â€“ ×›××• Pandas, ×”×›×™ × ×¤×•×¥ ×‘-PySpark |\n",
    "| `Dataset`    | (×œ× ×§×™×™× ×‘×¤×™×™×ª×•×Ÿ) â€“ ×©×™×œ×•×‘ ×©×œ DataFrame ×¢× ×˜×™×¤×•×¡×™ ××™×“×¢ ×‘-Java/Scala |\n",
    "\n",
    "### 3ï¸âƒ£ ×¡×¤×¨×™×•×ª ××ª×§×“××•×ª (Modules)\n",
    "| ×¡×¤×¨×™×™×”           | ×©×™××•×©                                       |\n",
    "|------------------|----------------------------------------------|\n",
    "| **Spark SQL**     | ×”×¨×¦×ª ×©××™×œ×ª×•×ª SQL ×¢×œ DataFrames             |\n",
    "| **MLlib**         | ×¡×¤×¨×™×™×” ×œ×œ××™×“×ª ××›×•× ×” ×‘×§× ×” ××™×“×” ×’×“×•×œ         |\n",
    "| **Spark Streaming** | ×¢×™×‘×•×“ ×¡×˜×¨×™××™× ×’ (Real Time Data)            |\n",
    "| **GraphX**        | × ×™×ª×•×— ×’×¨×¤×™× (×§×™×™× ×¨×§ ×‘-Scala)              |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ ×¢×§×¨×•× ×•×ª ×¢×‘×•×“×” ×—×©×•×‘×™× ×‘-Spark:\n",
    "\n",
    "### 1. SparkSession\n",
    "- \"××¨×›×– ×”×¤×™×§×•×“\" ×©×œ ××¤×œ×™×§×¦×™×™×ª Spark\n",
    "- ×“×¨×›×• ×™×•×¦×¨×™× DataFrame, ×˜×•×¢× ×™× ×§×‘×¦×™× ×•××¤×¢×™×œ×™× ××ª ×”×× ×•×¢\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "```\n",
    "\n",
    "### 2. Transformations (×˜×¨× ×¡×¤×•×¨××¦×™×•×ª)\n",
    "- ×¤×¢×•×œ×•×ª ×©××ª××¨×•×ª ××” ×œ×¢×©×•×ª ×¢×œ ×”× ×ª×•× ×™× (×›××• filter, groupBy)\n",
    "- **×œ× ××ª×‘×¦×¢×•×ª ×‘×¤×•×¢×œ** â€“ ×¨×§ × ×‘× ×” ×’×¨×£ ×©×œ ×¤×¢×•×œ×•×ª\n",
    "```python\n",
    "df_filtered = df.filter(df[\"age\"] > 30)  # ×¢×“×™×™×Ÿ ×œ× ×‘×•×¦×¢\n",
    "```\n",
    "\n",
    "### 3. Actions (×¤×¢×•×œ×•×ª ×¡×•×¤×™×•×ª)\n",
    "- ×’×•×¨××•×ª ×œ-Spark ×œ×”×¨×™×¥ ××ª ×›×œ ××” ×©×ª×™××¨× ×• ×œ×¤× ×™ ×›×Ÿ\n",
    "- ×“×•×’×××•×ª: `show()`, `count()`, `collect()`\n",
    "```python\n",
    "df_filtered.count()  # ×¢×›×©×™×• ××ª×‘×¦×¢ ×‘×¤×•×¢×œ\n",
    "```\n",
    "\n",
    "### 4. Lazy Evaluation (×”×¢×¨×›×” ×¢×¦×œ×”)\n",
    "- Spark **×œ× ××¨×™×¥ ×›×œ×•×** ×¢×“ ×©×¦×¨×™×š ×ª×•×¦××” ×¡×•×¤×™×ª (Action)\n",
    "- ×××¤×©×¨ ×œ×• ×œ×‘×¦×¢ ××•×¤×˜×™××™×–×¦×™×” ×•×œ×¢×‘×•×“ ×™×¢×™×œ ×™×•×ª×¨\n",
    "\n",
    "### 5. Partitions (××—×™×¦×•×ª)\n",
    "- Spark ××—×œ×§ ××ª ×”×“××˜×” ×œ×™×—×™×“×•×ª ×¢×‘×•×“×” (Partitions)\n",
    "- ××¨×™×¥ ×›×œ ×—×œ×§ ×‘××§×‘×™×œ (×× ×™×© ×›××” ×œ×™×‘×•×ª/××›×•× ×•×ª)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Spark ×œ×¢×•××ª Pandas:\n",
    "\n",
    "| × ×•×©×              | Pandas                           | Spark                            |\n",
    "|-------------------|----------------------------------|----------------------------------|\n",
    "| ×˜×¢×™× ×” ×œ×–×™×›×¨×•×Ÿ     | ×›×œ ×”×“××˜×” × ×˜×¢×Ÿ ×‘×‘×ª ××—×ª          | × ×˜×¢×Ÿ ×‘××—×™×¦×•×ª (Partitions)       |\n",
    "| ×’×•×“×œ ×“××˜×” ××ª××™×   | ×¢×“ ~1GB (×ª×œ×•×™ ×‘×–×™×›×¨×•×Ÿ)          | ××ª××™× ×’× ×œ×¢×©×¨×•×ª/×××•×ª GB         |\n",
    "| ××•×¤×Ÿ ×‘×™×¦×•×¢        | ××™×™×“×™ (Eager Execution)         | Lazy Evaluation (×“×—×•×™)           |\n",
    "| ×¨×™×¦×”              | ×œ×™×‘×” ××—×ª, ××—×©×‘ ××§×•××™            | ××§×‘×™×œ×™, ×ª×•××š ×‘-Cluster           |\n",
    "| ×‘×™×¦×•×¢×™× ×‘××—×©×‘ ××™×©×™| ××”×™×¨ ×××•×“ ×¢×œ ×§×‘×¦×™× ×§×˜× ×™×        | ××™×˜×™ ×™×—×¡×™×ª (×‘×’×œ×œ overhead)       |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª ××ª×™ ×œ×”×©×ª××© ×‘-Spark?\n",
    "- ×›×©×™×© **×“××˜×” ×¢×¦×•×** ×©×œ× × ×›× ×¡ ×œ×–×™×›×¨×•×Ÿ\n",
    "- ×›×©×¢×•×‘×“×™× ×‘×¢× ×Ÿ ××• ×‘Ö¾Cluster\n",
    "- ×›×©×¦×¨×™×š ×¢×™×‘×•×“ ××§×‘×™×œ×™, ××”×™×¨ ×•×××™×Ÿ ×‘×§× ×” ××™×“×” ×’×“×•×œ\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ ×”×¢×¨×” ×—×©×•×‘×”:\n",
    "> Spark **××™×˜×™ ×™×•×ª×¨ ××¤× ×“×¡ ×‘××—×©×‘ ××™×©×™** ×¢×œ ×§×‘×¦×™× ×§×˜× ×™×. ×”×•× ×¤×•×¨×— ×¨×§ ×›×©×¢×•×‘×“×™× ×¢× Big Data ×××™×ª×™ ×‘×¡×‘×™×‘×•×ª ××‘×•×–×¨×•×ª.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ×“×•×’××” ×§×¦×¨×”:\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", 25),\n",
    "    (\"Bob\", 17),\n",
    "    (\"Charlie\", 12),\n",
    "    (\"Dina\", 41),\n",
    "    (\"Eli\", 17),\n",
    "    (\"Tamar\", 33)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# groupBy ×œ×¤×™ ×’×™×œ ×•×¡×¤×™×¨×ª ××•×¤×¢×™×\n",
    "grouped = df.groupBy(\"age\").count()\n",
    "grouped.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ ×”×¦×¦×” ×¨××©×•× ×™×ª ×œ-SparkML:\n",
    "```python\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# ×“××˜×” ×¤×©×•×˜ ×œ×“×•×’××”\n",
    "data = [\n",
    "    (1.0, 2.0),\n",
    "    (2.0, 4.0),\n",
    "    (3.0, 6.0),\n",
    "    (4.0, 8.0)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"x\", \"y\"])\n",
    "\n",
    "# ×”×¤×™×›×ª x ×œ×•×•×§×˜×•×¨ ×ª×›×•× ×•×ª\n",
    "vec = VectorAssembler(inputCols=[\"x\"], outputCol=\"features\")\n",
    "df_vec = vec.transform(df)\n",
    "\n",
    "# ×‘× ×™×™×ª ××•×“×œ ×¨×’×¨×¡×™×” ×œ×™× ××¨×™×ª\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"y\")\n",
    "model = lr.fit(df_vec)\n",
    "\n",
    "# ×ª×¦×•×’×ª ××§×“××™×\n",
    "print(f\"××§×“× ×”×©×™×¤×•×¢: {model.coefficients[0]}\")\n",
    "print(f\"××§×“× ×”×—×™×ª×•×š: {model.intercept}\")\n",
    "```\n",
    "\n",
    "×¨×•×¦×” ×©× ×¨×—×™×‘ ××ª ×”×—×œ×§ ×©×œ SparkML ×¢× ×ª×¨×’×™×œ? ××• ×“×•×’××” ×¢× ×˜×¡×˜/×¤×¨×“×™×§×¦×™×”?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930d022-98c2-48c6-8924-d6f9c6e63f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
